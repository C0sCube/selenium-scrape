{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c7c1c57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyed Hash (HMAC-SHA1): 4cb62fcbc3810327973a632c2fb1c8c20d57ea82\n",
      "Un-Keyed Hash (HMAC-SHA1): 943a702d06f34599aee1f8da8ef9f7296031d699\n"
     ]
    }
   ],
   "source": [
    "import hmac\n",
    "import hashlib\n",
    "\n",
    "# Secret key and message\n",
    "key = b\"supersecretkey\"\n",
    "message = b\"Hello, world!\"\n",
    "\n",
    "# Create HMAC using SHA-256\n",
    "hmac_object = hmac.new(key, message, hashlib.sha1)\n",
    "hash = hashlib.sha1(message)\n",
    "# hmac_digest = hmac_object.hexdigest()\n",
    "\n",
    "print(\"Keyed Hash (HMAC-SHA1):\", hmac_object.hexdigest())\n",
    "print(\"Un-Keyed Hash (HMAC-SHA1):\", hash.hexdigest())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b7f28e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, base64, os, re,json5, pprint,random , math, hashlib\n",
    "import pandas as pd\n",
    "from selenium import webdriver                                # Controls the browser\n",
    "from selenium.webdriver.common.by import By                   # Locators (ID, CLASS_NAME, XPATH, etc.)\n",
    "from selenium.webdriver.support.ui import WebDriverWait       # Waits for elements to appear\n",
    "from selenium.webdriver.support import expected_conditions as EC  # Conditions like \"visible\", \"clickable\"\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from urllib.parse import urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "from app.utils import Helper\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a4250e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\kaustubh.keny\\Projects\\JSON25\\scrape_output\\data\\13082025\"\n",
    "\n",
    "# datastr = Helper.read_file(path)\n",
    "# df = pd.read_html(StringIO(datastr))[0]\n",
    "def fix_mojibake(text:str)->str:\n",
    "    try:\n",
    "        return text.encode(\"latin1\").decode(\"utf-8\")\n",
    "    except:\n",
    "        return text  # If it fails, return original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0b1087",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\kaustubh.keny\\Projects\\JSON25\\scrape_output\\data\\13082025\"\n",
    "banks = ['Bank of Baroda', 'Bank of India', 'Bank of Maharashtra', 'Canara Bank', 'Central Bank of India', 'Indian Bank', 'Indian Overseas Bank', 'Punjab and Sind Bank', 'Punjab National Bank', 'State Bank of India', 'Uco Bank', 'Union Bank of India']\n",
    "\n",
    "for bank in banks:\n",
    "    \n",
    "    print(bank)\n",
    "    dirs  =os.path.join(path,bank, \"save_html\")\n",
    "    if os.path.exists(dirs):\n",
    "        contents = os.listdir(dirs)\n",
    "        for content in contents:\n",
    "            html_path = os.path.join(dirs,content)\n",
    "            if not os.path.isfile(html_path):\n",
    "                continue\n",
    "            df  = pd.read_html(html_path)[0]\n",
    "            df = df.map(lambda x: x.lower() if isinstance(x,str) else x)\n",
    "            df = df.map(lambda x:str(x))\n",
    "            print(content)\n",
    "            if df.empty:\n",
    "                continue\n",
    "            id = \" \".join(df.iloc[0,:].to_list())\n",
    "            byte_id = id.encode(encoding=\"utf-8\")\n",
    "            print(hashlib.sha1(byte_id).hexdigest())\n",
    "\n",
    "    # for content in contents:\n",
    "    #     html_path = os.path.join(root,content)\n",
    "        \n",
    "    #     df  = pd.read_html(html_path)[0]\n",
    "    #     df = df.map(lambda x: x.lower() if isinstance(x,str) else x)\n",
    "    #     df = df.map(lambda x:str(x))\n",
    "    #     print(content)\n",
    "    #     id = \" \".join(df.iloc[0,:].to_list())\n",
    "    #     byte_id = id.encode(encoding=\"utf-8\")\n",
    "    #     print(hashlib.sha1(byte_id).hexdigest())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1d09add3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kaustubh.keny\\AppData\\Local\\Temp\\ipykernel_2972\\3130505306.py:3: DeprecationWarning: 'count' is passed as positional argument\n",
      "  dataStr = re.sub(r'<tr[^>]*>\\s*(?:&nbsp;|\\s)*</tr>|Â ',\"\",dataStr, re.IGNORECASE)\n",
      "C:\\Users\\kaustubh.keny\\AppData\\Local\\Temp\\ipykernel_2972\\3130505306.py:6: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(dataStr)[0]\n"
     ]
    }
   ],
   "source": [
    "path = r\"C:\\Users\\kaustubh.keny\\Projects\\JSON25\\scrape_output\\data\\13082025\\Punjab and Sind Bank\\save_html\\content_2.html\"\n",
    "dataStr = Helper.read_file(path)\n",
    "dataStr = re.sub(r'<tr[^>]*>\\s*(?:&nbsp;|\\s)*</tr>|Â ',\"\",dataStr, re.IGNORECASE)\n",
    "raw_html = re.sub(r'(Â[\\xa0\\s]*)+', ' ', dataStr)\n",
    "\n",
    "df = pd.read_html(dataStr)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "901f45d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"check.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c39f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json5\n",
    "from dateutil.parser import parse\n",
    "import re\n",
    "\n",
    "path = r'example.json5'\n",
    "\n",
    "def extract_date(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        print(\"Invalid data type. Expected string.\")\n",
    "        return \"\"\n",
    "    \n",
    "    output_format = \"%Y%m%d\"\n",
    "    date_pattern = r\"(\\d{2}[.\\-/]+\\d{2}[.\\-/]+\\d{4}|\\d{1,2}\\s*(?:th|st|rd|nd)\\s*[A-Za-z]+\\s*\\d{4}|\\d{2}[\\.\\-\\/]+[A-Za-z]+[\\.\\-\\/]+\\d{4}|\\d{2}\\s*[A-Za-z]+\\s*\\d{4})\"\n",
    "    matches = re.findall(date_pattern, text, re.IGNORECASE)\n",
    "    \n",
    "    if matches:\n",
    "        date_str = \" \".join(matches)\n",
    "        dt_object = parse(date_str, fuzzy=True)\n",
    "        return dt_object.strftime(output_format)\n",
    "    \n",
    "    # print(\"Date not found.\")\n",
    "    return text\n",
    "\n",
    "with open(path, \"r+\", encoding=\"utf-8\") as file:\n",
    "    data = json5.load(file)\n",
    "\n",
    "records = data.get(\"records\", [])\n",
    "\n",
    "for record in records:\n",
    "    \n",
    "    print(record[\"bank_name\"])\n",
    "    response_data = record.get(\"response\", {})\n",
    "    if not response_data:\n",
    "        continue\n",
    "\n",
    "    extracted_dates = {}\n",
    "\n",
    "    for block in response_data.values():\n",
    "        scrape_data = block.get(\"scrape_data\", {})\n",
    "        if not scrape_data:\n",
    "            continue\n",
    "\n",
    "        cleaned_data = {key: extract_date(value) for key, value in scrape_data.items()}\n",
    "        extracted_dates.update(cleaned_data)\n",
    "\n",
    "    print(extracted_dates)\n",
    "    record[\"response\"] = extracted_dates\n",
    "\n",
    "with open(\"up20250801.json5\", \"r+\", encoding=\"utf-8\") as file:\n",
    "   json5.dump(data,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f75797",
   "metadata": {},
   "outputs": [],
   "source": [
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def human_scroll(driver, total_scroll=2000, step=50):\n",
    "    for y in range(0, total_scroll, step):\n",
    "        driver.execute_script(f\"window.scrollTo(0, {y});\")\n",
    "        time.sleep(random.uniform(0.05, 0.3))\n",
    "\n",
    "\n",
    "def human_move_mouse(driver, start=(0, 0), end=(300, 300), steps=30):\n",
    "    actions = ActionChains(driver)\n",
    "    x1, y1 = start\n",
    "    x2, y2 = end\n",
    "    for t in range(steps):\n",
    "        progress = t / steps\n",
    "        x = x1 + (x2 - x1) * progress + random.uniform(-2, 2)\n",
    "        y = y1 + (y2 - y1) * progress + math.sin(progress * math.pi) * 20\n",
    "        actions.move_by_offset(int(x), int(y))\n",
    "    actions.perform()\n",
    "\n",
    "\n",
    "def human_click(elem):\n",
    "    time.sleep(random.uniform(0.2, 1.5))\n",
    "    elem.click()\n",
    "\n",
    "\n",
    "# ==== SETUP CHROME WITH GOOGLE PROFILE ====\n",
    "profile_path = r\"C:\\Users\\rando\\AppData\\Local\\Google\\Chrome\\User Data\"\n",
    "profile_dir = \"Profile 1\" # or \"Default\"\n",
    "\n",
    "options = uc.ChromeOptions()\n",
    "options.add_argument(f\"--user-data-dir={profile_path}\")\n",
    "options.add_argument(f\"--profile-directory={profile_dir}\")\n",
    "options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "options.add_argument(\"--start-maximized\")\n",
    "\n",
    "driver = uc.Chrome(options=options)\n",
    "\n",
    "# ==== GO TO SITE ====\n",
    "driver.get(\"https://www.unionbankofindia.co.in/en/details/rate-of-interest\")\n",
    "\n",
    "# ==== SIMULATE HUMAN BEHAVIOR ====\n",
    "human_scroll(driver, total_scroll=1500, step=70)\n",
    "human_move_mouse(driver, start=(0, 0), end=(200, 200), steps=40)\n",
    "\n",
    "try:\n",
    "    table = driver.find_element(By.TAG_NAME, \"table\")\n",
    "    human_click(table) # Just demo click\n",
    "    print(\"Table found:\", table.get_attribute(\"outerHTML\")[:200])\n",
    "except Exception as e:\n",
    "    print(\"No table found:\", e)\n",
    "\n",
    "time.sleep(5)\n",
    "driver.quit()\n",
    "\n",
    "import undetected_chromedriver as uc \n",
    "\n",
    "profile_path = r\"C:\\Users\\kaustubh.keny\\AppData\\Local\\Google\\Chrome\\User Data\"\n",
    "profile_dir = \"Default\" # Change to match your actual one\n",
    "\n",
    "options = uc.ChromeOptions()\n",
    "options.add_argument(f\"--user-data-dir={profile_path}\")\n",
    "options.add_argument(f\"--profile-directory={profile_dir}\")\n",
    "# options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "options.add_argument(\"--start-maximized\")\n",
    "\n",
    "driver = uc.Chrome(driver_executable_path=r\"C:\\Users\\kaustubh.keny\\chromedriver-win64\\chromedriver.exe\",options=options)\n",
    "driver.get(\"https://www.unionbankofindia.co.in/en/details/rate-of-interest\")\n",
    "\n",
    "# import scrapy\n",
    "\n",
    "# class UnionBankSpider(scrapy.Spider):\n",
    "#     name = \"unionbank_tables\"\n",
    "#     allowed_domains = [\"unionbankofindia.co.in\"]\n",
    "#     start_urls = [\"https://www.unionbankofindia.co.in/en/details/rate-of-interest\"]\n",
    "\n",
    "#     def parse(self, response):\n",
    "#         tables = response.css(\"table\").getall()\n",
    "#         self.logger.info(f\"Found {len(tables)} tables\")\n",
    "#         for idx, table in enumerate(tables):\n",
    "#             yield {\n",
    "#                 \"table_index\": idx,\n",
    "#                 \"html\": table\n",
    "#             }\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
