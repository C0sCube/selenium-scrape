{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b7f28e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import time, base64,ssl, os, re,json5, pprint,random , math, hashlib, inspect,requests\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.by import By                   # Locators (ID, CLASS_NAME, XPATH, etc.)\n",
    "from selenium.webdriver.support.ui import WebDriverWait       # Waits for elements to appear\n",
    "from selenium.webdriver.support import expected_conditions as EC  # Conditions like \"visible\", \"clickable\"\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from urllib.parse import urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "from app.utils import Helper\n",
    "from app.operation_executor import OperationExecutor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from io import StringIO\n",
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08adf686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch 2 with 20 links\n",
      "[FAIL] Flexi Growth Fund\n",
      "[21] Failed: Message: invalid session id: session deleted as the browser has closed the connection\n",
      "from disconnected: not connected to DevTools\n",
      "  (Session info: chrome=140.0.7339.208); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#invalidsessionidexception\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x0x7ff7119c1eb5+80197]\n",
      "\tGetHandleVerifier [0x0x7ff7119c1f10+80288]\n",
      "\t(No symbol) [0x0x7ff7117402fa]\n",
      "\t(No symbol) [0x0x7ff71172c0a5]\n",
      "\t(No symbol) [0x0x7ff71175144a]\n",
      "\t(No symbol) [0x0x7ff7117c7be6]\n",
      "\t(No symbol) [0x0x7ff7117e8132]\n",
      "\t(No symbol) [0x0x7ff7117c0153]\n",
      "\t(No symbol) [0x0x7ff711788b02]\n",
      "\t(No symbol) [0x0x7ff7117898d3]\n",
      "\tGetHandleVerifier [0x0x7ff711c7e83d+2949837]\n",
      "\tGetHandleVerifier [0x0x7ff711c78c6a+2926330]\n",
      "\tGetHandleVerifier [0x0x7ff711c986c7+3055959]\n",
      "\tGetHandleVerifier [0x0x7ff7119dcfee+191102]\n",
      "\tGetHandleVerifier [0x0x7ff7119e50af+224063]\n",
      "\tGetHandleVerifier [0x0x7ff7119caf64+117236]\n",
      "\tGetHandleVerifier [0x0x7ff7119cb119+117673]\n",
      "\tGetHandleVerifier [0x0x7ff7119b10a8+11064]\n",
      "\tBaseThreadInitThunk [0x0x7ffc3706e8d7+23]\n",
      "\tRtlUserThreadStart [0x0x7ffc378a8d9c+44]\n",
      "\n"
     ]
    },
    {
     "ename": "InvalidSessionIdException",
     "evalue": "Message: invalid session id; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#invalidsessionidexception\nStacktrace:\n\tGetHandleVerifier [0x0x7ff7119c1eb5+80197]\n\tGetHandleVerifier [0x0x7ff7119c1f10+80288]\n\t(No symbol) [0x0x7ff71174011c]\n\t(No symbol) [0x0x7ff711787c1d]\n\t(No symbol) [0x0x7ff7117c0242]\n\t(No symbol) [0x0x7ff7117bac14]\n\t(No symbol) [0x0x7ff7117b9cca]\n\t(No symbol) [0x0x7ff71170a755]\n\tGetHandleVerifier [0x0x7ff711c7e83d+2949837]\n\tGetHandleVerifier [0x0x7ff711c78c6a+2926330]\n\tGetHandleVerifier [0x0x7ff711c986c7+3055959]\n\tGetHandleVerifier [0x0x7ff7119dcfee+191102]\n\tGetHandleVerifier [0x0x7ff7119e50af+224063]\n\t(No symbol) [0x0x7ff711709731]\n\tGetHandleVerifier [0x0x7ff711dab568+4182008]\n\tBaseThreadInitThunk [0x0x7ffc3706e8d7+23]\n\tRtlUserThreadStart [0x0x7ffc378a8d9c+44]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidSessionIdException\u001b[39m                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 174\u001b[39m\n\u001b[32m    172\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] Failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[43mdriver\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwindow_handles\u001b[49m) > \u001b[32m1\u001b[39m:\n\u001b[32m    175\u001b[39m         driver.switch_to.window(driver.window_handles[-\u001b[32m1\u001b[39m])\n\u001b[32m    176\u001b[39m         driver.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kaustubh.keny\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:640\u001b[39m, in \u001b[36mWebDriver.window_handles\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    632\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m    633\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwindow_handles\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    634\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Returns the handles of all windows within the current session.\u001b[39;00m\n\u001b[32m    635\u001b[39m \n\u001b[32m    636\u001b[39m \u001b[33;03m    Example:\u001b[39;00m\n\u001b[32m    637\u001b[39m \u001b[33;03m    --------\u001b[39;00m\n\u001b[32m    638\u001b[39m \u001b[33;03m    >>> print(driver.window_handles)\u001b[39;00m\n\u001b[32m    639\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m640\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[43m.\u001b[49m\u001b[43mW3C_GET_WINDOW_HANDLES\u001b[49m\u001b[43m)\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kaustubh.keny\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:458\u001b[39m, in \u001b[36mWebDriver.execute\u001b[39m\u001b[34m(self, driver_command, params)\u001b[39m\n\u001b[32m    455\u001b[39m response = cast(RemoteConnection, \u001b[38;5;28mself\u001b[39m.command_executor).execute(driver_command, params)\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[32m--> \u001b[39m\u001b[32m458\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43merror_handler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    459\u001b[39m     response[\u001b[33m\"\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m._unwrap_value(response.get(\u001b[33m\"\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    460\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kaustubh.keny\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:232\u001b[39m, in \u001b[36mErrorHandler.check_response\u001b[39m\u001b[34m(self, response)\u001b[39m\n\u001b[32m    230\u001b[39m         alert_text = value[\u001b[33m\"\u001b[39m\u001b[33malert\u001b[39m\u001b[33m\"\u001b[39m].get(\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    231\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[31mInvalidSessionIdException\u001b[39m: Message: invalid session id; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#invalidsessionidexception\nStacktrace:\n\tGetHandleVerifier [0x0x7ff7119c1eb5+80197]\n\tGetHandleVerifier [0x0x7ff7119c1f10+80288]\n\t(No symbol) [0x0x7ff71174011c]\n\t(No symbol) [0x0x7ff711787c1d]\n\t(No symbol) [0x0x7ff7117c0242]\n\t(No symbol) [0x0x7ff7117bac14]\n\t(No symbol) [0x0x7ff7117b9cca]\n\t(No symbol) [0x0x7ff71170a755]\n\tGetHandleVerifier [0x0x7ff711c7e83d+2949837]\n\tGetHandleVerifier [0x0x7ff711c78c6a+2926330]\n\tGetHandleVerifier [0x0x7ff711c986c7+3055959]\n\tGetHandleVerifier [0x0x7ff7119dcfee+191102]\n\tGetHandleVerifier [0x0x7ff7119e50af+224063]\n\t(No symbol) [0x0x7ff711709731]\n\tGetHandleVerifier [0x0x7ff711dab568+4182008]\n\tBaseThreadInitThunk [0x0x7ffc3706e8d7+23]\n\tRtlUserThreadStart [0x0x7ffc378a8d9c+44]\n"
     ]
    }
   ],
   "source": [
    "import time, base64, os, re\n",
    "import pandas as pd\n",
    "from selenium import webdriver                                # Controls the browser\n",
    "from selenium.webdriver.common.by import By                   # Locators (ID, CLASS_NAME, XPATH, etc.)\n",
    "from selenium.webdriver.support.ui import WebDriverWait       # Waits for elements to appear\n",
    "from selenium.webdriver.support import expected_conditions as EC  # Conditions like \"visible\", \"clickable\"\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from urllib.parse import urljoin\n",
    " \n",
    "SELECTORS = {\n",
    "    \"filter_list\": (By.ID, \"filterlist\"),\n",
    "    \"last_filter_li\": (By.XPATH, \"./li[last()]\"),\n",
    "    \"fund_links\": (By.CSS_SELECTOR, \"#fund-perform-table a\"),\n",
    "    \"portfolio_button\": (By.CLASS_NAME, \"portfolio\"),\n",
    "    \"show_more\": (By.ID, \"showMore1\"),\n",
    "    \"companies_table\": (By.ID, \"companies-table\"),\n",
    "    \"page_header\": (By.TAG_NAME, \"h1\"),\n",
    "    \"tab_content\": (By.CLASS_NAME, \"tab-content\")\n",
    "}\n",
    " \n",
    "BASE_URL = r\"https://www.iciciprulife.com/fund-performance/all-products-fund-performance-details.html\"\n",
    "PDF_FOLDER = \"INSR_PDF\"\n",
    "XLS_FILE = \"INSURANCE_MASTER_XLS.xlsx\"\n",
    "BATCH_SIZE = 20\n",
    "COOLDOWN = 30\n",
    " \n",
    "os.makedirs(PDF_FOLDER, exist_ok=True)\n",
    " \n",
    " \n",
    "# --- Utility Functions ---\n",
    "def clean_text(text):\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'[\\\\/*?:\"<>|\\/]', \"_\", text.lower())\n",
    "    return text\n",
    " \n",
    "def save_pdf(driver,filename):\n",
    "    pdf = driver.execute_cdp_cmd(\"Page.printToPDF\", {\n",
    "        \"printBackground\": True,\n",
    "        \"paperWidth\": 8.27,\n",
    "        \"paperHeight\": 11.69\n",
    "    })\n",
    "   \n",
    "    save_path = os.path.join(PDF_FOLDER, f\"{filename}.pdf\")\n",
    "    with open(save_path, \"wb\") as f:\n",
    "        f.write(base64.b64decode(pdf['data']))\n",
    " \n",
    "def save_table_to_excel(driver, content, writer):\n",
    "    tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "    all_data = []\n",
    " \n",
    "    for table in tables:\n",
    "        rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "        for row in rows:\n",
    "            cells = row.find_elements(By.XPATH, \".//th | .//td\")\n",
    "            row_data = [cell.text.strip() for cell in cells]\n",
    "            if any(row_data):\n",
    "                all_data.append(row_data)\n",
    "        all_data.append([])\n",
    " \n",
    "    if all_data:\n",
    "        df = pd.DataFrame(all_data)\n",
    "        df.insert(0,\"url\",content[\"url\"])\n",
    "        df.insert(1,\"ins\",content[\"name\"])\n",
    "        df.insert(2,\"sfin\",content[\"sfin\"])\n",
    "        df.to_excel(writer, sheet_name=content[\"name\"][:31], index=False, header=False)\n",
    "         \n",
    "    # file_path = os.path.join(CONFIG[\"xls_folder\"], filename)\n",
    "    # df.to_excel(file_path, index=False, header=False)\n",
    " \n",
    "def execute_scripts(driver, element=None, type=\"\"):\n",
    "    if type == \"\":\n",
    "        print(\"Type not Correct\")\n",
    "        return driver\n",
    " \n",
    "    REQ_SCRIPTS = {\n",
    "        \"color\": \"\"\"\n",
    "            const style = document.createElement('style');\n",
    "            style.type = 'text/css';\n",
    "            style.innerHTML = `\n",
    "                html, body {\n",
    "                    background: white !important;\n",
    "                    color: #111 !important;\n",
    "                    -webkit-print-color-adjust: exact !important;\n",
    "                    print-color-adjust: exact !important;\n",
    "                }\n",
    "                * {\n",
    "                    color: #111 !important;\n",
    "                    background: white !important;\n",
    "                    border-color: #111 !important;\n",
    "                }\n",
    "                a, span, div, td, th, p, h1, h2, h3, h4, h5, h6 {\n",
    "                    color: #111 !important;\n",
    "                }\n",
    "            `;\n",
    "            document.head.appendChild(style);\n",
    "        \"\"\"\n",
    "    }\n",
    " \n",
    "    driverReturn = driver.execute_script(REQ_SCRIPTS[type], element) if element else driver.execute_script(REQ_SCRIPTS[type])\n",
    "    time.sleep(1)\n",
    "    return driverReturn\n",
    " \n",
    "# --- Browser Setup ---\n",
    "options = Options()\n",
    "options.add_argument(\"start-maximized\")\n",
    "options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36\")\n",
    "# options.add_argument(\"--headless\")\n",
    "driver = webdriver.Chrome(options=options)\n",
    "driver.get(BASE_URL)\n",
    " \n",
    "excelWriter = pd.ExcelWriter(XLS_FILE,engine=\"xlsxwriter\")\n",
    " \n",
    "WebDriverWait(driver, 5).until(EC.presence_of_element_located(SELECTORS[\"tab_content\"]))\n",
    " \n",
    "filter_list = driver.find_element(*SELECTORS[\"filter_list\"])\n",
    "last_all = filter_list.find_element(*SELECTORS[\"last_filter_li\"])\n",
    "last_all.click()\n",
    "time.sleep(2)\n",
    " \n",
    "pdf_links = driver.find_elements(*SELECTORS[\"fund_links\"])\n",
    " \n",
    "total_links = len(pdf_links)\n",
    "print(\"Total links:\", total_links)\n",
    " \n",
    "for batch_start in range(0, total_links, BATCH_SIZE):\n",
    "    batch = pdf_links[batch_start:batch_start + BATCH_SIZE]\n",
    "    print(f\"\\nProcessing batch {(batch_start // BATCH_SIZE) + 1} with {len(batch)} links\")\n",
    " \n",
    "    for j, link in enumerate(batch):\n",
    "        i = batch_start + j\n",
    "        try:\n",
    "            driver.execute_script(\"window.open(arguments[0].href, '_blank');\", link)\n",
    "            WebDriverWait(driver, 5).until(lambda d: len(d.window_handles) == 2)\n",
    "            driver.switch_to.window(driver.window_handles[-1])\n",
    "           \n",
    "            try:\n",
    "                performance_element = WebDriverWait(driver, 8).until(\n",
    "                    EC.element_to_be_clickable(SELECTORS[\"portfolio_button\"])\n",
    "                )\n",
    "                performance_element.click()\n",
    "                WebDriverWait(driver, 8).until(lambda d: len(d.window_handles) == 3)\n",
    "                driver.switch_to.window(driver.window_handles[-1])\n",
    "                time.sleep(2)\n",
    "            except:\n",
    "               \n",
    "                print(\"Portfolio click failed or not present\")\n",
    "           \n",
    "            try:driver.find_element(*SELECTORS[\"show_more\"]).click()\n",
    "            except:driver.execute_script(\"document.getElementById('showMore1').click();\")\n",
    " \n",
    "            WebDriverWait(driver, 8).until(\n",
    "                EC.presence_of_element_located(SELECTORS[\"companies_table\"])\n",
    "            )\n",
    " \n",
    "            execute_scripts(driver, type=\"color\")\n",
    "       \n",
    "            header = driver.find_element(*SELECTORS[\"page_header\"]).text\n",
    "            file_name, sfin = header.split(\"\\n\")\n",
    "            content = {\n",
    "                \"url\":driver.current_url,\n",
    "                \"name\": clean_text(file_name),\n",
    "                \"sfin\": sfin\n",
    "            }\n",
    "           \n",
    "            print(f\"[{i+1}] INSR NAME: {content[\"name\"]} PDF URL: {content[\"url\"]}\")\n",
    " \n",
    "            save_pdf(driver,file_name)\n",
    "            save_table_to_excel(driver,content,excelWriter)\n",
    " \n",
    "        except Exception as e:\n",
    "            print(f\"[FAIL] {file_name}\")\n",
    "            print(f\"[{i+1}] Failed: {e}\")\n",
    "        finally:\n",
    "            while len(driver.window_handles) > 1:\n",
    "                driver.switch_to.window(driver.window_handles[-1])\n",
    "                driver.close()\n",
    "            driver.switch_to.window(driver.window_handles[0])\n",
    " \n",
    "    if batch_start + BATCH_SIZE < total_links:\n",
    "        print(f\"Sleeping {COOLDOWN} seconds before next batch...\")\n",
    "        time.sleep(COOLDOWN)\n",
    "       \n",
    "excelWriter.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96ce952",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    " \n",
    "session = requests.Session()\n",
    " \n",
    "session.headers.update({\n",
    "    # 'Authorization': 'Bearer YOUR_API_TOKEN',  # if needed\n",
    "    'Accept': 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'\n",
    "})\n",
    " \n",
    " \n",
    "from datetime import datetime, timedelta\n",
    " \n",
    "def get_date_range(start_date, end_date):\n",
    "    start = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "    delta = (end - start).days + 1\n",
    "    return [(start + timedelta(days=i)).strftime(\"%Y-%m-%d\") for i in range(delta)]\n",
    " \n",
    "# Example usage\n",
    "dates = get_date_range(\"2019-04-01\", \"2025-03-31\")\n",
    "# print(dates)\n",
    " \n",
    " \n",
    "dates = ['2025-08-15', '2025-08-16', '2025-08-17', '2025-08-18', '2025-08-19', '2025-08-20', '2025-08-21', '2025-08-22', '2025-08-23', '2025-08-24']\n",
    " \n",
    "for idx, value in enumerate(dates[:-1]):\n",
    "    print(dates[idx],dates[idx+1])\n",
    "   \n",
    "    start_date,end_date = dates[idx],dates[idx+1]\n",
    "    url = f\"https://www.amfiindia.com/api/download-nav-history?strMFID=all&FromDate={start_date}&ToDate={start_date}\"\n",
    " \n",
    "    # Make the request\n",
    "    response = session.get(url, verify=False)\n",
    " \n",
    "    # Check if request was successful\n",
    "    if response.status_code == 200:\n",
    "        file_name = f\"{start_date}.xlsx\"\n",
    "        with open(file_name, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(\"✅ Excel file downloaded successfully.\")\n",
    "    else:\n",
    "        print(f\"❌ Failed to download. Status code: {response.status_code}\")\n",
    "   \n",
    "    time.sleep(10)\n",
    " \n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    " \n",
    " \n",
    "TODAY = datetime.now().strftime('%Y%m%d')\n",
    " \n",
    "with open(\"config.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    " \n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(f\"nav_downloader{TODAY}.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    " \n",
    "os.makedirs(config[\"download_path\"], exist_ok=True)\n",
    " \n",
    " \n",
    "session = requests.Session()\n",
    "session.headers.update({\n",
    "    'Accept': 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'\n",
    "})\n",
    " \n",
    "def get_date_range(start_date, end_date):\n",
    "    start = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "    delta = (end - start).days + 1\n",
    "    return [(start + timedelta(days=i)).strftime(\"%Y-%m-%d\") for i in range(delta)]\n",
    " \n",
    "def download_nav_for_date(date):\n",
    "    url = f\"{config['base_url']}?strMFID=all&FromDate={date}&ToDate={date}\"\n",
    "    headers = config.get(\"headers\",{})\n",
    "    logging.info(f\"Requesting NAV data for {date}\")\n",
    "   \n",
    "    try:\n",
    "        response = session.get(url, verify=config[\"verify_ssl\"], headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            file_path = os.path.join(config[\"download_path\"], f\"{date}.xlsx\")\n",
    "            with open(file_path, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            logging.info(f\" Downloaded: {file_path}\")\n",
    "        else:\n",
    "            logging.warning(f\" Failed for {date}. Status code: {response.status_code}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Exception occurred for {date}: {e}\")\n",
    " \n",
    "def main():\n",
    "    dates = get_date_range(config[\"start_date\"], config[\"end_date\"])\n",
    "    for date in dates:\n",
    "        download_nav_for_date(date)\n",
    "        time.sleep(config[\"sleep_seconds\"])\n",
    " \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    " \n",
    "{\n",
    "  \"download_path\": \"./downloads\",\n",
    "  \"start_date\": \"2025-08-15\",\n",
    "  \"end_date\": \"2025-08-17\",\n",
    "  \"base_url\": \"https://www.amfiindia.com/api/download-nav-history\",\n",
    "  \"verify_ssl\": false,\n",
    "  \"sleep_seconds\": 4,\n",
    "  \"headers\":{\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\",\n",
    "    \"Accept\": \"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\",\n",
    "    \"Referer\": \"https://www.amfiindia.com/\"\n",
    "}\n",
    "}\n",
    " \n",
    "@echo off\n",
    "REM Dir\n",
    "cd /d \"C:\\Users\\kaustubh.keny\\Projects\\office-work\\law-tribunal\"\n",
    " \n",
    "REM Program Run\n",
    "python nav_downloader.py\n",
    " \n",
    "REM Pause to keep the window open\n",
    "pause\n",
    " \n",
    "hi\n",
    " \n",
    "wfh aahes ?\n",
    " \n",
    "hi \n",
    "mi ek code dila toh run karshil ka ?? python cha aahe\n",
    " \n",
    "nhi\n",
    " \n",
    "office madhe ahe\n",
    " \n",
    "ha de na code\n",
    " \n",
    "karto run\n",
    " \n",
    "import requests\n",
    "import pandas as pd\n",
    "from io import BytesIO,StringIO\n",
    " \n",
    "session = requests.Session()\n",
    "session.headers.update({\"User-Agent\": \"Mozilla/5.0\"})\n",
    " \n",
    "def fetch_rba():\n",
    "    url = \"https://www.rba.gov.au/statistics/tables/xls/f01d.xlsx\"\n",
    "    r = session.get(url, timeout=15)\n",
    "    r.raise_for_status()\n",
    "   \n",
    "    df = pd.read_excel(BytesIO(r.content))\n",
    "    df_T = pd.DataFrame(df.iloc[-5:].values, columns=df.iloc[1].values).T\n",
    "    # df_T = df.iloc[-5:].T.reset_index()\n",
    "    df_T.columns = df_T.iloc[0]\n",
    "    df_T.drop(0, inplace=True)\n",
    "    # df_T = df_T.drop(0).set_index(df_T.columns[0])\n",
    "    return df_T\n",
    " \n",
    "def fetch_bcra():\n",
    "    url = \"https://www.bcra.gob.ar/PublicacionesEstadisticas/Principales_variables_i.asp\"\n",
    "    r = session.get(url, timeout=15)\n",
    "    r.raise_for_status()\n",
    "    return pd.read_html(r.text)[0]\n",
    " \n",
    "def fetch_banxico(token, series_ids, start, end):\n",
    "   \n",
    "    #mapper\n",
    "    series_map = {\n",
    "        \"SF61745\": \"Target rate 1/\",\n",
    "        \"SF331451\": \"Overnight TIIE funding rate 2/\",\n",
    "        \"SF43783\": \"28 day TIIE 3/\",\n",
    "        \"SF43878\": \"91 day TIIE 3/\",\n",
    "        \"SF111916\": \"182 day TIIE 3/\"\n",
    "    }\n",
    "   \n",
    "    #api call\n",
    "    url = f\"https://www.banxico.org.mx/SieAPIRest/service/v1/series/{','.join(series_ids)}/datos/{start}/{end}\"\n",
    "    headers = {\"Bmx-Token\": token}\n",
    "    r = session.get(url, headers=headers, timeout=15)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    " \n",
    "    records = {}\n",
    "    for s in data['bmx']['series']:\n",
    "        sid = s['idSerie']\n",
    "        rec = {d['fecha']: d['dato'] for d in s['datos']}\n",
    "        records[sid] = rec\n",
    "    df = pd.DataFrame.from_dict(records, orient=\"index\")\n",
    "    df.columns = pd.to_datetime(df.columns, dayfirst=True)\n",
    "   \n",
    "    df.index = df.index.map(lambda x: series_map.get(x, x))\n",
    "    # print(df)\n",
    "   \n",
    "    return df.sort_index(axis=1)\n",
    " \n",
    "def fetch_boc(series_ids, start, end):\n",
    "   \n",
    "    #mapper\n",
    "    series_map={\n",
    "        \"V39079\":\"Target for the overnight rate\",\n",
    "        \"CL.CDN.MOST.1DL\":\"Overnight money market financing rate1\",\n",
    "        \"V39078\":\"Bank rate - Daily\",\n",
    "        \"V80691310\":\"Bank rate - Weekly\",\n",
    "        \"V122530\":\"Bank rate - Monthly\"\n",
    "    }\n",
    "   \n",
    "    #api call\n",
    "    url = f\"https://www.bankofcanada.ca/valet/observations/{','.join(series_ids)}/json?start_date={start}&end_date={end}\"\n",
    "    r = session.get(url, timeout=15)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    " \n",
    "    records = {}\n",
    "    for row in data[\"observations\"]:\n",
    "        date = row[\"d\"]\n",
    "        for sid in series_ids:\n",
    "            records.setdefault(sid, {})[date] = row.get(sid, {}).get(\"v\")\n",
    "    df = pd.DataFrame.from_dict(records, orient=\"index\")\n",
    "    df.columns = pd.to_datetime(df.columns)\n",
    "   \n",
    "    df.index = df.index.map(lambda x: series_map.get(x, x))\n",
    "   \n",
    "    return df.sort_index(axis=1)\n",
    " \n",
    "with pd.ExcelWriter(\"central_banks.xlsx\") as writer:\n",
    "    token = \"1846d489e18513d4306b70e257a21edb56e82325de0139af91dd2176975029bf\"\n",
    "    fetch_banxico(token, [\"SF61745\",\"SF331451\",\"SF43783\",\"SF43878\",\"SF111916\"], \"2025-09-18\", \"2025-09-24\")\\\n",
    "        .to_excel(writer, sheet_name=\"Banxico\")\n",
    "    fetch_boc([\"V39079\",\"CL.CDN.MOST.1DL\",\"V39078\",\"V80691310\",\"V122530\"], \"2025-09-18\", \"2025-09-24\")\\\n",
    "        .to_excel(writer, sheet_name=\"BoC\")\n",
    "    # fetch_rba().to_excel(writer, sheet_name=\"RBA\")\n",
    "    fetch_bcra().to_excel(writer, sheet_name=\"BCRA\", index=False)\n",
    " \n",
    "notebook madhe kar \n",
    " \n",
    "central_banks.xlsx\n",
    " \n",
    "tyms\n",
    " \n",
    "import time, base64, os, re\n",
    "import pandas as pd\n",
    "from selenium import webdriver                                # Controls the browser\n",
    "from selenium.webdriver.common.by import By                   # Locators (ID, CLASS_NAME, XPATH, etc.)\n",
    "from selenium.webdriver.support.ui import WebDriverWait       # Waits for elements to appear\n",
    "from selenium.webdriver.support import expected_conditions as EC  # Conditions like \"visible\", \"clickable\"\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from urllib.parse import urljoin\n",
    " \n",
    "SELECTORS = {\n",
    "    \"filter_list\": (By.ID, \"filterlist\"),\n",
    "    \"last_filter_li\": (By.XPATH, \"./li[last()]\"),\n",
    "    \"fund_links\": (By.CSS_SELECTOR, \"#fund-perform-table a\"),\n",
    "    \"portfolio_button\": (By.CLASS_NAME, \"portfolio\"),\n",
    "    \"show_more\": (By.ID, \"showMore1\"),\n",
    "    \"companies_table\": (By.ID, \"companies-table\"),\n",
    "    \"page_header\": (By.TAG_NAME, \"h1\"),\n",
    "    \"tab_content\": (By.CLASS_NAME, \"tab-content\")\n",
    "}\n",
    " \n",
    "BASE_URL = r\"https://www.iciciprulife.com/fund-performance/all-products-fund-performance-details.html\"\n",
    "PDF_FOLDER = \"INSR_PDF\"\n",
    "XLS_FILE = \"INSURANCE_MASTER_XLS.xlsx\"\n",
    "BATCH_SIZE = 20\n",
    "COOLDOWN = 30\n",
    " \n",
    "os.makedirs(PDF_FOLDER, exist_ok=True)\n",
    " \n",
    " \n",
    "# --- Utility Functions ---\n",
    "def clean_text(text):\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'[\\\\/*?:\"<>|\\/]', \"_\", text.lower())\n",
    "    return text\n",
    " \n",
    "def save_pdf(driver,filename):\n",
    "    pdf = driver.execute_cdp_cmd(\"Page.printToPDF\", {\n",
    "        \"printBackground\": True,\n",
    "        \"paperWidth\": 8.27,\n",
    "        \"paperHeight\": 11.69\n",
    "    })\n",
    "   \n",
    "    save_path = os.path.join(PDF_FOLDER, f\"{filename}.pdf\")\n",
    "    with open(save_path, \"wb\") as f:\n",
    "        f.write(base64.b64decode(pdf['data']))\n",
    " \n",
    "def save_table_to_excel(driver, content, writer):\n",
    "    tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "    all_data = []\n",
    " \n",
    "    for table in tables:\n",
    "        rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "        for row in rows:\n",
    "            cells = row.find_elements(By.XPATH, \".//th | .//td\")\n",
    "            row_data = [cell.text.strip() for cell in cells]\n",
    "            if any(row_data):\n",
    "                all_data.append(row_data)\n",
    "        all_data.append([])\n",
    " \n",
    "    if all_data:\n",
    "        df = pd.DataFrame(all_data)\n",
    "        df.insert(0,\"url\",content[\"url\"])\n",
    "        df.insert(1,\"ins\",content[\"name\"])\n",
    "        df.insert(2,\"sfin\",content[\"sfin\"])\n",
    "        df.to_excel(writer, sheet_name=content[\"name\"][:31], index=False, header=False)\n",
    "         \n",
    "    # file_path = os.path.join(CONFIG[\"xls_folder\"], filename)\n",
    "    # df.to_excel(file_path, index=False, header=False)\n",
    " \n",
    "def execute_scripts(driver, element=None, type=\"\"):\n",
    "    if type == \"\":\n",
    "        print(\"Type not Correct\")\n",
    "        return driver\n",
    " \n",
    "    REQ_SCRIPTS = {\n",
    "        \"color\": \"\"\"\n",
    "            const style = document.createElement('style');\n",
    "            style.type = 'text/css';\n",
    "            style.innerHTML = `\n",
    "                html, body {\n",
    "                    background: white !important;\n",
    "                    color: #111 !important;\n",
    "                    -webkit-print-color-adjust: exact !important;\n",
    "                    print-color-adjust: exact !important;\n",
    "                }\n",
    "                * {\n",
    "                    color: #111 !important;\n",
    "                    background: white !important;\n",
    "                    border-color: #111 !important;\n",
    "                }\n",
    "                a, span, div, td, th, p, h1, h2, h3, h4, h5, h6 {\n",
    "                    color: #111 !important;\n",
    "                }\n",
    "            `;\n",
    "            document.head.appendChild(style);\n",
    "        \"\"\"\n",
    "    }\n",
    " \n",
    "    driverReturn = driver.execute_script(REQ_SCRIPTS[type], element) if element else driver.execute_script(REQ_SCRIPTS[type])\n",
    "    time.sleep(1)\n",
    "    return driverReturn\n",
    " \n",
    "# --- Browser Setup ---\n",
    "options = Options()\n",
    "options.add_argument(\"start-maximized\")\n",
    "options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36\")\n",
    "options.add_argument(\"--headless\")\n",
    "driver = webdriver.Chrome(options=options)\n",
    "driver.get(BASE_URL)\n",
    " \n",
    "excelWriter = pd.ExcelWriter(XLS_FILE,engine=\"xlsxwriter\")\n",
    " \n",
    "WebDriverWait(driver, 5).until(EC.presence_of_element_located(SELECTORS[\"tab_content\"]))\n",
    " \n",
    "filter_list = driver.find_element(*SELECTORS[\"filter_list\"])\n",
    "last_all = filter_list.find_element(*SELECTORS[\"last_filter_li\"])\n",
    "last_all.click()\n",
    "time.sleep(2)\n",
    " \n",
    "pdf_links = driver.find_elements(*SELECTORS[\"fund_links\"])\n",
    " \n",
    "total_links = len(pdf_links)\n",
    "print(\"Total links:\", total_links)\n",
    " \n",
    "for batch_start in range(0, total_links, BATCH_SIZE):\n",
    "    batch = pdf_links[batch_start:batch_start + BATCH_SIZE]\n",
    "    print(f\"\\nProcessing batch {(batch_start // BATCH_SIZE) + 1} with {len(batch)} links\")\n",
    " \n",
    "    for j, link in enumerate(batch):\n",
    "        i = batch_start + j\n",
    "        try:\n",
    "            driver.execute_script(\"window.open(arguments[0].href, '_blank');\", link)\n",
    "            WebDriverWait(driver, 5).until(lambda d: len(d.window_handles) == 2)\n",
    "            driver.switch_to.window(driver.window_handles[-1])\n",
    "           \n",
    "            try:\n",
    "                performance_element = WebDriverWait(driver, 8).until(\n",
    "                    EC.element_to_be_clickable(SELECTORS[\"portfolio_button\"])\n",
    "                )\n",
    "                performance_element.click()\n",
    "                WebDriverWait(driver, 8).until(lambda d: len(d.window_handles) == 3)\n",
    "                driver.switch_to.window(driver.window_handles[-1])\n",
    "                time.sleep(2)\n",
    "            except:\n",
    "               \n",
    "                print(\"Portfolio click failed or not present\")\n",
    "           \n",
    "            try:driver.find_element(*SELECTORS[\"show_more\"]).click()\n",
    "            except:driver.execute_script(\"document.getElementById('showMore1').click();\")\n",
    " \n",
    "            WebDriverWait(driver, 8).until(\n",
    "                EC.presence_of_element_located(SELECTORS[\"companies_table\"])\n",
    "            )\n",
    " \n",
    "            execute_scripts(driver, type=\"color\")\n",
    "       \n",
    "            header = driver.find_element(*SELECTORS[\"page_header\"]).text\n",
    "            file_name, sfin = header.split(\"\\n\")\n",
    "            content = {\n",
    "                \"url\":driver.current_url,\n",
    "                \"name\": clean_text(file_name),\n",
    "                \"sfin\": sfin\n",
    "            }\n",
    "           \n",
    "            print(f\"[{i+1}] INSR NAME: {content[\"name\"]} PDF URL: {content[\"url\"]}\")\n",
    " \n",
    "            save_pdf(driver,file_name)\n",
    "            save_table_to_excel(driver,content,excelWriter)\n",
    " \n",
    "        except Exception as e:\n",
    "            print(f\"[FAIL] {file_name}\")\n",
    "            print(f\"[{i+1}] Failed: {e}\")\n",
    "        finally:\n",
    "            while len(driver.window_handles) > 1:\n",
    "                driver.switch_to.window(driver.window_handles[-1])\n",
    "                driver.close()\n",
    "            driver.switch_to.window(driver.window_handles[0])\n",
    " \n",
    "    if batch_start + BATCH_SIZE < total_links:\n",
    "        print(f\"Sleeping {COOLDOWN} seconds before next batch...\")\n",
    "        time.sleep(COOLDOWN)\n",
    "       \n",
    "excelWriter.close()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3436ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Define start and end dates\n",
    "start_date = datetime(2021, 1, 1)\n",
    "end_date = datetime.today()\n",
    "\n",
    "month_ends = pd.date_range(start=start_date, end=end_date, freq='M')\n",
    "last_5_days = []\n",
    "for date in month_ends:\n",
    "    for i in range(4, -1, -1):  # Reverse to get earlier days first\n",
    "        day = date - pd.Timedelta(days=i)\n",
    "        formatted = day.strftime('%d-%b-%Y').lower()\n",
    "        last_5_days.append(formatted)\n",
    "\n",
    "last_5_days.sort(key=lambda x: datetime.strptime(x, '%d-%b-%Y'))\n",
    "for d in last_5_days:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf5d185",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from datetime import datetime\n",
    "\n",
    "# dates = [\n",
    "#     \"27-sep-2025\", \"28-sep-2025\", \"29-sep-2025\", \"30-sep-2025\", \"01-oct-2025\",\n",
    "# ]\n",
    "all_data = []\n",
    "\n",
    "not_found = []\n",
    "\n",
    "for dato in last_5_days:\n",
    "    url = f\"https://www.amfiindia.com/api/tracking-error-data?MF_ID=all&strdt={dato}\"\n",
    "    response = requests.get(url, verify=False)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        if not data[\"data\"]:\n",
    "            not_found.append(f\"No Data Found for date: {dato}\")\n",
    "        df = pd.DataFrame(data[\"data\"])\n",
    "        df[\"date\"] = dato \n",
    "        all_data.append(df)\n",
    "    \n",
    "    time.sleep(3)\n",
    "\n",
    "\n",
    "with open(\"not_there.txt\",\"w\") as f:\n",
    "    f.writelines(not_found)\n",
    "\n",
    "final_df = pd.concat(all_data, ignore_index=True)\n",
    "final_df.to_excel(\"MutualFundSchemes.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d591581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are less than three cols, skipping\n",
      "Excel saved: Orders_example_All.xlsx\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def parse_html_table(html):\n",
    "    \"\"\"Parse one HTML table into DataFrame with Date, Subject, Remarks, Link.\"\"\"\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    table = soup.find(\"table\")\n",
    "    if not table:\n",
    "        return None\n",
    "\n",
    "    rows = []\n",
    "    for tr in table.select(\"tbody tr\"):\n",
    "        cols = tr.find_all(\"td\")\n",
    "        if len(cols) < 3:\n",
    "            print(\"There are less than three cols, skipping\")\n",
    "            continue\n",
    "\n",
    "        date = cols[0].get_text(strip=True)\n",
    "        remarks = cols[2].get_text(strip=True)\n",
    "\n",
    "        a = cols[1].find(\"a\")\n",
    "        subject = a.get_text(\" \", strip=True) if a else cols[1].get_text(strip=True)\n",
    "        link = \"\"\n",
    "        if a and a.get(\"onclick\"):\n",
    "            onclick = a[\"onclick\"]\n",
    "            if \"newwindow1\" in onclick:\n",
    "                start = onclick.find(\"'\") + 1\n",
    "                end = onclick.rfind(\"'\")\n",
    "                link = onclick[start:end]\n",
    "\n",
    "        rows.append([date, subject, remarks, link])\n",
    "\n",
    "    return pd.DataFrame(rows, columns=[\"Date\", \"Subject\", \"Remarks\", \"Link\"])\n",
    "\n",
    "def clean_sheet_name(name, fallback=\"Sheet\"):\n",
    "    safe = re.sub(r'[\\[\\]\\:\\*\\?\\/\\\\]', \"_\", str(name))\n",
    "    return safe[:31] if safe else fallback\n",
    "\n",
    "def cache_to_excel(cache_file, excel_out=\"Orders_All.xlsx\"):\n",
    "    with open(cache_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    used_names = {}  # track counts per sheet name\n",
    "\n",
    "    with pd.ExcelWriter(excel_out, engine=\"xlsxwriter\") as writer:\n",
    "        for scraped in data[\"records\"][0][\"scraped_data\"]:\n",
    "            if not scraped.get(\"data_present\"):\n",
    "                continue\n",
    "\n",
    "            action = scraped.get(\"action\", \"unknown\")\n",
    "            webpage = scraped.get(\"webpage\", \"\")\n",
    "            for resp in scraped.get(\"response\", []):\n",
    "                if resp.get(\"type\") != \"table_html\":\n",
    "                    continue\n",
    "\n",
    "                df = parse_html_table(resp[\"value\"])\n",
    "                if df is None or df.empty:\n",
    "                    continue\n",
    "\n",
    "                titles = resp.get(\"title\", [])\n",
    "                base_name = clean_sheet_name(titles[-1] if titles else action)\n",
    "\n",
    "                count = used_names.get(base_name, 0) + 1\n",
    "                used_names[base_name] = count\n",
    "                sheet_name = base_name if count == 1 else clean_sheet_name(f\"{base_name}_{count}\")\n",
    "\n",
    "\n",
    "                meta_rows = [[\"Action\", action], [\"Webpage\", webpage]]\n",
    "                for i, t in enumerate(titles, start=1):\n",
    "                    meta_rows.append([f\"Header{i}\", t])\n",
    "                meta = pd.DataFrame(meta_rows, columns=[\"Meta\", \"Value\"])\n",
    "                meta.to_excel(writer, sheet_name=sheet_name, index=False, startrow=0)\n",
    "\n",
    "                df_visible = df[[\"Date\", \"Subject\", \"Remarks\",\"Link\"]]\n",
    "                startrow = len(meta) + 2\n",
    "                df_visible.to_excel(writer, sheet_name=sheet_name, index=False, startrow=startrow)\n",
    "\n",
    "                # ws = writer.sheets[sheet_name]\n",
    "                # for i, (txt, url) in enumerate(zip(df[\"Subject\"], df[\"Link\"]), start=startrow+1):\n",
    "                #     if url:\n",
    "                #         ws.write_url(i, 1, url, string=txt)\n",
    "\n",
    "    print(f\"Excel saved: {excel_out}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "path = r\"C:\\Users\\kaustubh.keny\\Projects\\JSON25\\scrape_output\\cache\\2025-09-23\\25-09-23T15-32-09_cache.json\"\n",
    "cache_to_excel(path, \"Orders_example_All.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e62817d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.operation_executor import OperationExecutor\n",
    "from app.utils import Helper\n",
    "path = r\"C:\\Users\\rando\\Office Projects\\outputs\\scrape_output\\cache\\2025-09-23\\25-09-23T22-31-06_cache.json\"\n",
    "data = Helper.load_json(path)\n",
    "OperationExecutor.generate_cache_doc_report(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b827023e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from io import BytesIO,StringIO\n",
    "from datetime import datetime\n",
    "\n",
    "session = requests.Session()\n",
    "session.headers.update({\"User-Agent\": \"Mozilla/5.0\"})\n",
    "\n",
    "def fetch_rba(): \n",
    "    url = \"https://www.rba.gov.au/statistics/tables/xls/f01d.xlsx\" \n",
    "    r = session.get(url, timeout=15) \n",
    "    r.raise_for_status() \n",
    "    df = pd.read_excel(BytesIO(r.content)) \n",
    "    df.columns = df.iloc[1, :]\n",
    "    df = df.iloc[-5:, :].T.reset_index()\n",
    "    df.columns = df.iloc[0, :]\n",
    "    df = df.drop(df.index[0])\n",
    "    df.columns = [(lambda c: pd.to_datetime(c, errors=\"ignore\"))(c) for c in df.columns]\n",
    "    df.columns = [c.strftime(\"%m-%d-%Y\") if isinstance(c, pd.Timestamp) else c for c in df.columns]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def fetch_bcra():\n",
    "    url = \"https://www.bcra.gob.ar/PublicacionesEstadisticas/Principales_variables_i.asp\"\n",
    "    r = session.get(url, timeout=15, verify=False)\n",
    "    r.raise_for_status()\n",
    "    df = pd.read_html(r.text)[0]\n",
    "    \n",
    "    today = datetime.today().strftime(\"%m/%d/%Y\")\n",
    "    date_range = pd.date_range(end=today, periods=5)   # datetime index\n",
    "\n",
    "    df.columns = [\"Header\",\"Date\",\"Value\"]\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\", dayfirst=True)\n",
    "\n",
    "    reshaped = (df.pivot_table(index=\"Header\", columns=\"Date\", values=\"Value\", aggfunc=\"first\"))\n",
    "    reshaped = reshaped.reindex(columns=date_range)\n",
    "    reshaped.columns = [d.strftime(\"%m-%d-%Y\") for d in reshaped.columns]\n",
    "    reshaped = reshaped.reset_index()\n",
    "    return reshaped\n",
    "\n",
    "def fetch_banxico(token, series_ids, start, end): \n",
    "    #mapper\n",
    "    series_map = {\n",
    "        \"SF61745\": \"Target rate 1/\",\n",
    "        \"SF331451\": \"Overnight TIIE funding rate 2/\",\n",
    "        \"SF43783\": \"28 day TIIE 3/\",\n",
    "        \"SF43878\": \"91 day TIIE 3/\",\n",
    "        \"SF111916\": \"182 day TIIE 3/\"\n",
    "    }\n",
    "    \n",
    "    #api call\n",
    "    url = f\"https://www.banxico.org.mx/SieAPIRest/service/v1/series/{','.join(series_ids)}/datos/{start}/{end}\"\n",
    "    headers = {\"Bmx-Token\": token}\n",
    "    r = session.get(url, headers=headers, timeout=15)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "\n",
    "    records = {}\n",
    "    for s in data['bmx']['series']:\n",
    "        sid = s['idSerie']\n",
    "        rec = {d['fecha']: d['dato'] for d in s['datos']}\n",
    "        records[sid] = rec\n",
    "    df = pd.DataFrame.from_dict(records, orient=\"index\")\n",
    "    df.columns = pd.to_datetime(df.columns, dayfirst=True)\n",
    "    \n",
    "    df.index = df.index.map(lambda x: series_map.get(x, x))\n",
    "    # print(df)\n",
    "    df.columns = [d.strftime(\"%m-%d-%Y\") for d in df.columns]\n",
    "    \n",
    "    return df.sort_index(axis=1)\n",
    "\n",
    "def fetch_boc(series_ids, start, end):\n",
    "    series_map = {\n",
    "        \"V39079\": \"Target for the overnight rate\",\n",
    "        \"CL.CDN.MOST.1DL\": \"Overnight money market financing rate1\",\n",
    "        \"V39078\": \"Bank rate - Daily\",\n",
    "        \"V80691310\": \"Bank rate - Weekly\",\n",
    "        \"V122530\": \"Bank rate - Monthly\"\n",
    "    }\n",
    "    \n",
    "    url = f\"https://www.bankofcanada.ca/valet/observations/{','.join(series_ids)}/json?start_date={start}&end_date={end}\"\n",
    "    r = session.get(url, timeout=15)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "\n",
    "    records = {}\n",
    "    for row in data[\"observations\"]:\n",
    "        date = row[\"d\"]\n",
    "        for sid in series_ids:\n",
    "            records.setdefault(sid, {})[date] = row.get(sid, {}).get(\"v\")\n",
    "\n",
    "    df = pd.DataFrame.from_dict(records, orient=\"index\")\n",
    "    # make sure column labels are datetime\n",
    "    df.columns = pd.to_datetime(df.columns)\n",
    "    full_range = pd.date_range(start=start, end=end, freq=\"D\")\n",
    "    df = df.reindex(columns=full_range)\n",
    "    df.columns = [d.strftime(\"%m-%d-%Y\") for d in df.columns]\n",
    "    df.index = df.index.map(lambda x: series_map.get(x, x))\n",
    "\n",
    "    return df.sort_index(axis=1)\n",
    "\n",
    "\n",
    "with pd.ExcelWriter(\"central_banks_1.xlsx\") as writer:\n",
    "    # token = \"1846d489e18513d4306b70e257a21edb56e82325de0139af91dd2176975029bf\"\n",
    "    # fetch_banxico(token, [\"SF61745\",\"SF331451\",\"SF43783\",\"SF43878\",\"SF111916\"], \"2025-09-25\", \"2025-09-29\")\\\n",
    "    #     .to_excel(writer, sheet_name=\"Banxico\")\n",
    "    # fetch_boc([\"V39079\",\"CL.CDN.MOST.1DL\",\"V39078\",\"V80691310\",\"V122530\"], \"2025-09-25\", \"2025-09-29\")\\\n",
    "    #     .to_excel(writer, sheet_name=\"BoC\")\n",
    "    # fetch_rba().to_excel(writer, sheet_name=\"RBA\", index = False)\n",
    "    fetch_bcra().to_excel(writer, sheet_name=\"BCRA\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18f28dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kaustubh.keny\\AppData\\Local\\Temp\\ipykernel_21004\\3839537942.py:59: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(r.text)[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to Daily Interest Rate.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests, re, shutil, os\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "from datetime import datetime, timedelta\n",
    "from bs4 import BeautifulSoup\n",
    "from openpyxl import load_workbook\n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "session = requests.Session()\n",
    "session.headers.update({\"User-Agent\": \"Mozilla/5.0\"})\n",
    "\n",
    "\n",
    "# ---- Shared Pandas Helpers ----\n",
    "def normalize_columns_to_dates(df):\n",
    "    \"\"\"Convert datetime-like columns to mm-dd-YYYY strings.\"\"\"\n",
    "    df.columns = [\n",
    "        c.strftime(\"%m-%d-%Y\") if isinstance(c, pd.Timestamp) else c\n",
    "        for c in df.columns\n",
    "    ]\n",
    "    return df\n",
    "\n",
    "def ensure_full_date_range(df, start, end):\n",
    "    \"\"\"Ensure dataframe has full date coverage, fill missing dates with NaN.\"\"\"\n",
    "    full_range = pd.date_range(start=start, end=end, freq=\"D\")\n",
    "    df = df.reindex(columns=full_range)\n",
    "    return normalize_columns_to_dates(df)\n",
    "\n",
    "def format_header(header):\n",
    "    if isinstance(header, (pd.Timestamp, datetime)):\n",
    "        return header.strftime(\"%d%m%Y\")\n",
    "    if not isinstance(header, str):\n",
    "        return str(header)\n",
    "\n",
    "    for fmt in (\"%Y-%m-%d\", \"%Y-%m-%d %H:%M:%S\", \"%y%m%d\", \"%m/%d/%Y\", \"%d-%m-%Y\", \"%d/%m/%Y\", \"%m-%d-%Y\"):\n",
    "        try:\n",
    "            parsed = datetime.strptime(header, fmt)\n",
    "            return parsed.strftime(\"%d%m%Y\")\n",
    "        except ValueError:\n",
    "            continue\n",
    "    return header\n",
    "    return header\n",
    "# ---- Fetchers ----\n",
    "def fetch_rba(cfg, start=None, end=None):\n",
    "    r = session.get(cfg[\"url\"], timeout=15)\n",
    "    r.raise_for_status()\n",
    "    df = pd.read_excel(BytesIO(r.content))\n",
    "\n",
    "    df.columns = df.iloc[1, :]\n",
    "    df = df.iloc[-5:, :].T.reset_index()\n",
    "    df.columns = df.iloc[0, :]\n",
    "    df = df.drop(df.index[0])\n",
    "\n",
    "    return normalize_columns_to_dates(df)\n",
    "\n",
    "def fetch_bcra(cfg, start=None, end=None):\n",
    "    r = session.get(cfg[\"url\"], timeout=15, verify=False)\n",
    "    r.raise_for_status()\n",
    "    df = pd.read_html(r.text)[0]\n",
    "\n",
    "    today = datetime.today().strftime(\"%m/%d/%Y\")\n",
    "    date_range = pd.date_range(end=today, periods=5)   # last 5 days\n",
    "\n",
    "    df.columns = [\"Header\", \"Date\", \"Value\"]\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\", dayfirst=True)\n",
    "\n",
    "    reshaped = df.pivot_table(index=\"Header\", columns=\"Date\", values=\"Value\", aggfunc=\"first\")\n",
    "    reshaped = reshaped.reindex(columns=date_range)\n",
    "    reshaped = normalize_columns_to_dates(reshaped)\n",
    "    return reshaped.reset_index()\n",
    "\n",
    "def fetch_banxico(cfg,start, end): \n",
    "\n",
    "    series_map = cfg.get(\"series_map\")\n",
    "    series_ids = cfg.get(\"series_ids\")\n",
    "    headers =  cfg.get(\"headers\")\n",
    "    base_url =  cfg.get(\"url\")\n",
    "    #api call\n",
    "    url = f\"{base_url}/{','.join(series_ids)}/datos/{start}/{end}\"\n",
    "    r = session.get(url, headers=headers, timeout=15)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "\n",
    "    records = {}\n",
    "    for s in data['bmx']['series']:\n",
    "        sid = s['idSerie']\n",
    "        rec = {d['fecha']: d['dato'] for d in s['datos']}\n",
    "        records[sid] = rec\n",
    "    df = pd.DataFrame.from_dict(records, orient=\"index\")\n",
    "    df.columns = pd.to_datetime(df.columns, dayfirst=True)\n",
    "    \n",
    "    df.index = df.index.map(lambda x: series_map.get(x, x))\n",
    "    df.columns = [d.strftime(\"%m-%d-%Y\") for d in df.columns]\n",
    "    return df.sort_index(axis=1)\n",
    "\n",
    "def fetch_boc(cfg, start, end):\n",
    "    series_map = cfg.get(\"series_map\")\n",
    "    series_ids = cfg.get(\"series_ids\")\n",
    "    base_url =  cfg.get(\"url\")\n",
    "    \n",
    "    url = f\"{base_url}/{','.join(series_ids)}/json?start_date={start}&end_date={end}\"\n",
    "    r = session.get(url, timeout=15)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "\n",
    "    records = {}\n",
    "    for row in data[\"observations\"]:\n",
    "        date = row[\"d\"]\n",
    "        for sid in series_ids:\n",
    "            records.setdefault(sid, {})[date] = row.get(sid, {}).get(\"v\")\n",
    "\n",
    "    df = pd.DataFrame.from_dict(records, orient=\"index\")\n",
    "    # make sure column labels are datetime\n",
    "    df.columns = pd.to_datetime(df.columns)\n",
    "    full_range = pd.date_range(start=start, end=end, freq=\"D\")\n",
    "    df = df.reindex(columns=full_range)\n",
    "    df.columns = [d.strftime(\"%m-%d-%Y\") for d in df.columns]\n",
    "    df.index = df.index.map(lambda x: series_map.get(x, x))\n",
    "\n",
    "    return df.sort_index(axis=1)\n",
    "\n",
    "def fetch_boj(cfg, start, end):\n",
    "    dates = pd.date_range(start=datetime.strptime(start, \"%y%m%d\"), end=datetime.strptime(end, \"%y%m%d\"))\n",
    "    records = {}\n",
    "\n",
    "    for date in dates:\n",
    "        url = f\"{cfg['url']}/md{date.strftime('%y%m%d')}.htm\"\n",
    "        try:\n",
    "            r = session.get(url, timeout=15)\n",
    "            if r.status_code != 200:continue\n",
    "        except Exception:continue\n",
    "\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "        for li in soup.find_all(\"li\"):\n",
    "            text = li.get_text(strip=True)\n",
    "            text = re.sub(r\"[\\[\\%]+\",\"\",text,re.IGNORECASE)\n",
    "            parts = text.split(\"]\")\n",
    "            if len(parts) < 2:continue\n",
    "            key,val = parts\n",
    "            # print(f\"{key}:{val}\")\n",
    "            series_name = f\"Call Rate {key}\"\n",
    "            records.setdefault(series_name, {})[date] = val\n",
    "\n",
    "    out = pd.DataFrame.from_dict(records, orient=\"index\")\n",
    "    out = out.reindex(columns=dates)\n",
    "    return normalize_columns_to_dates(out)\n",
    "\n",
    "def fetch_nyfed(cfg, start, end):\n",
    "    url = (\n",
    "        f\"{cfg['url']}?startDt={start}&endDt={end}\"\n",
    "        f\"&sort={cfg['headers']['sort']}\"\n",
    "        f\"&productCode={cfg['headers']['productCode']}\"\n",
    "        f\"&eventCodes={cfg['headers']['eventCodes']}\"\n",
    "        f\"&format={cfg['headers']['format']}\"\n",
    "    )\n",
    "    r = session.get(url, timeout=15)\n",
    "    r.raise_for_status()\n",
    "\n",
    "    if cfg[\"headers\"][\"format\"] == \"csv\":\n",
    "        df = pd.read_csv(BytesIO(r.content))\n",
    "    else:\n",
    "        df = pd.read_excel(BytesIO(r.content))\n",
    "\n",
    "    df.columns = [str(c).strip() for c in df.columns]\n",
    "    return df\n",
    "\n",
    "def fetch_cboe(cfg, start=None, end=None, days=5):\n",
    "    r = session.get(cfg[\"url\"], timeout=15, verify=False)\n",
    "    r.raise_for_status()\n",
    "\n",
    "    df = pd.read_csv(BytesIO(r.content))\n",
    "    df[\"DATE\"] = pd.to_datetime(df[\"DATE\"], errors=\"coerce\")\n",
    "    df = df.iloc[-days:, :]\n",
    "    start_dt = pd.to_datetime(start)\n",
    "    end_dt = pd.to_datetime(end)\n",
    "    full_range = pd.date_range(start=start_dt, end=end_dt, freq=\"D\")\n",
    "    df = df.set_index(\"DATE\").reindex(full_range)\n",
    "    wide = pd.DataFrame([df[\"OVX\"].tolist()], index=[\"OVX Index\"], columns=full_range)\n",
    "    return normalize_columns_to_dates(wide)\n",
    "\n",
    "CONFIG = {\n",
    "    \"Banxico\": {\n",
    "        \"fetcher\": fetch_banxico,\n",
    "        \"url\": \"https://www.banxico.org.mx/SieAPIRest/service/v1/series\",\n",
    "        \"headers\": {\n",
    "            \"Bmx-Token\": \"1846d489e18513d4306b70e257a21edb56e82325de0139af91dd2176975029bf\"\n",
    "        },\n",
    "        \"series_ids\":[\"SF61745\",\"SF331451\",\"SF43783\",\"SF43878\",\"SF111916\"],\n",
    "        \"series_map\": {\n",
    "            \"SF61745\": \"Target rate 1/\",\n",
    "            \"SF331451\": \"Overnight TIIE funding rate 2/\",\n",
    "            \"SF43783\": \"28 day TIIE 3/\",\n",
    "            \"SF43878\": \"91 day TIIE 3/\",\n",
    "            \"SF111916\": \"182 day TIIE 3/\"\n",
    "        }\n",
    "    },\n",
    "    \"BoC\": {\n",
    "        \"fetcher\": fetch_boc,\n",
    "        \"url\": \"https://www.bankofcanada.ca/valet/observations\",\n",
    "        \"series_ids\": [\"V39079\",\"CL.CDN.MOST.1DL\",\"V39078\",\"V80691310\",\"V122530\"],\n",
    "        \"series_map\": {\n",
    "            \"V39079\": \"Target for the overnight rate\",\n",
    "            \"CL.CDN.MOST.1DL\": \"Overnight money market financing rate1\",\n",
    "            \"V39078\": \"Bank rate - Daily\",\n",
    "            \"V80691310\": \"Bank rate - Weekly\",\n",
    "            \"V122530\": \"Bank rate - Monthly\"\n",
    "        }\n",
    "    },\n",
    "    \"RBA\": {\n",
    "        \"fetcher\": fetch_rba,\n",
    "        \"url\": \"https://www.rba.gov.au/statistics/tables/xls/f01d.xlsx\"\n",
    "    },\n",
    "    \"BCRA\": {\n",
    "        \"fetcher\": fetch_bcra,\n",
    "        \"url\": \"https://www.bcra.gob.ar/PublicacionesEstadisticas/Principales_variables_i.asp\"\n",
    "    },\n",
    "    \"BOJ\": {\n",
    "        \"fetcher\": fetch_boj,\n",
    "        \"url\": \"https://www3.boj.or.jp/market/en/stat\"\n",
    "    },\n",
    "    \"NYCFED\": {\n",
    "        \"fetcher\": fetch_nyfed,\n",
    "        \"url\": \"https://markets.newyorkfed.org/read\",\n",
    "        \"headers\": {\n",
    "            \"sort\": \"postDt:-1,'data.closeTm':-1\",\n",
    "            \"productCode\": 70,     # repo operations\n",
    "            \"eventCodes\": 730,     # event code for operations\n",
    "            \"format\": \"csv\"        # NY Fed supports csv or xls\n",
    "        }\n",
    "    },\n",
    "    \"CBOE\": {\n",
    "        \"fetcher\": fetch_cboe,\n",
    "        \"url\": \"https://cdn.cboe.com/api/global/us_indices/daily_prices/OVX_History.csv\"\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def run_pipeline(days=5, outfile=\"Daily Interest Rate.xlsx\"):\n",
    "    end = datetime.today()\n",
    "    start = end - timedelta(days=days)\n",
    "\n",
    "    with pd.ExcelWriter(outfile) as writer:\n",
    "        for name, cfg in CONFIG.items():\n",
    "            fetcher = cfg[\"fetcher\"]\n",
    "            try:\n",
    "                if name in [\"Banxico\"]:\n",
    "                    df = fetcher(cfg, start.strftime(\"%Y-%m-%d\"), end.strftime(\"%Y-%m-%d\"))\n",
    "                elif name in [\"BoC\"]:\n",
    "                    df = fetcher(cfg, start.strftime(\"%Y-%m-%d\"), end.strftime(\"%Y-%m-%d\"))\n",
    "                elif name in [\"BOJ\"]:\n",
    "                    df = fetcher(cfg, start.strftime(\"%y%m%d\"), end.strftime(\"%y%m%d\"))\n",
    "                elif name in [\"NYCFED\"]:\n",
    "                    df = fetcher(cfg, start.strftime(\"%Y-%m-%d\"), end.strftime(\"%Y-%m-%d\"))\n",
    "                elif name in [\"CBOE\"]:\n",
    "                    df = fetcher(cfg, start.strftime(\"%m/%d/%Y\"), end.strftime(\"%m/%d/%Y\"))\n",
    "                else:\n",
    "                    df = fetcher(cfg, start, end)\n",
    "                if df is not None and not df.empty:\n",
    "                    df.columns = [format_header(d) for d in df.columns]\n",
    "                    df.to_excel(writer, sheet_name=name, index=not name in [\"RBA\", \"BCRA\",\"NYCFED\"])\n",
    "                else:\n",
    "                    print(f\"{name}: No data returned\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching {name}: {e}\")\n",
    "\n",
    "    print(f\"Data saved to {outfile}\")\n",
    "\n",
    "# def run_pipeline(days=5, outfile=\"Daily Interest Rate.xlsx\"):\n",
    "#     end = datetime.today()\n",
    "#     start = end - timedelta(days=days)\n",
    "\n",
    "#     # Step 1: Backup the existing file\n",
    "#     if os.path.exists(outfile):\n",
    "#         timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "#         backup_file = outfile.replace(\".xlsx\", f\"_backup_{timestamp}.xlsx\")\n",
    "#         shutil.copy2(outfile, backup_file)\n",
    "#         print(f\"Backup created: {backup_file}\")\n",
    "\n",
    "#     # Step 2: Update sheets using ExcelWriter in append mode\n",
    "#     for name, cfg in CONFIG.items():\n",
    "#         fetcher = cfg[\"fetcher\"]\n",
    "#         try:\n",
    "#             if name in [\"Banxico\", \"BoC\", \"NYCFED\"]:\n",
    "#                 df = fetcher(cfg, start.strftime(\"%Y-%m-%d\"), end.strftime(\"%Y-%m-%d\"))\n",
    "#             elif name == \"BOJ\":\n",
    "#                 df = fetcher(cfg, start.strftime(\"%y%m%d\"), end.strftime(\"%y%m%d\"))\n",
    "#             elif name == \"CBOE\":\n",
    "#                 df = fetcher(cfg, start.strftime(\"%m/%d/%Y\"), end.strftime(\"%m/%d/%Y\"))\n",
    "#             else:\n",
    "#                 df = fetcher(cfg, start, end)\n",
    "\n",
    "#             if df is not None and not df.empty:\n",
    "#                 df.columns = [format_header(d) for d in df.columns]\n",
    "\n",
    "#                 # Remove the sheet if it exists\n",
    "#                 book = load_workbook(outfile)\n",
    "#                 if name in book.sheetnames:\n",
    "#                     std = book[name]\n",
    "#                     book.remove(std)\n",
    "#                     book.save(outfile)\n",
    "\n",
    "#                 # Write updated sheet\n",
    "#                 with pd.ExcelWriter(outfile, engine=\"openpyxl\", mode=\"a\", if_sheet_exists=\"replace\") as writer:\n",
    "#                     df.to_excel(writer, sheet_name=name, index=not name in [\"RBA\", \"BCRA\", \"NYCFED\"])\n",
    "#             else:\n",
    "#                 print(f\"{name}: No data returned\")\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error fetching {name}: {e}\")\n",
    "\n",
    "#     print(f\"Data updated in {outfile}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_pipeline(days=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff1a9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = r\"https://markets.newyorkfed.org/read?startDt=2025-09-23&endDt=2025-09-27&sort=postDt:-1,'data.closeTm':-1&productCode=70&eventCodes=730&format=csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2eff7757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>Processing Bank of Baroda\n"
     ]
    }
   ],
   "source": [
    "from app.operation_executor import OperationExecutor\n",
    "from app.utils import Helper\n",
    "from urllib.parse import urljoin\n",
    "import os\n",
    "ops = OperationExecutor()\n",
    "path = r\"C:\\Users\\kaustubh.keny\\Projects\\JSON25\\scrape_output\\cache\\2025-09-26\\25-09-26T15-44-07_cache.json\"\n",
    "# path = r\"25-09-08T18-33-09_cache.json5\"\n",
    "path = r\"C:\\Users\\kaustubh.keny\\Projects\\JSON25\\scrape_output\\cache\\2025-09-26\\25-09-26T15-39-53_cache.json\"\n",
    "\n",
    "    \n",
    "root_file = os.path.basename(path).split(\"_\")[0]\n",
    "function_to_execute = {\n",
    "\"primary\":[[\"normalize_df\",\"value\",\"norm_table\",\"table_html\"],[\"sha1\",\"value\",\"SHA_ONE\",\"pdf\"]],\n",
    "\"secondary\":[[\"sha1\",\"norm_table\",\"SHA_ONE\"]],\n",
    "}\n",
    "try:\n",
    "    data = Helper.load_json(path, typ=\"json5\")\n",
    "    processed_data = ops.runner(data,function_to_execute)\n",
    "    Helper.save_json(processed_data,f\"{root_file}_process.json\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError in Processing.. Skipping, {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "945fdec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Despoit_Rate_Report.xlsx'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from app.operation_executor import OperationExecutor\n",
    "from app.utils import Helper\n",
    "ops = OperationExecutor()\n",
    "\n",
    "v1 = r\"25-09-26T15-44-07_process.json\"\n",
    "v2 = r\"25-09-26T15-39-53_process.json\"\n",
    "yesterday = Helper.load_json(v2)\n",
    "today = Helper.load_json(v1)\n",
    "\n",
    "result = ops.process_comparison(yesterday,today,key=\"SHA_ONE\")\n",
    "Helper.save_json(result, \"compare-080925.json\")\n",
    "\n",
    "ops.generate_sorted_excel_report(result,output_path=\"Despoit_Rate_Report.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87a26a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Despoit_Rate_Report.xlsx'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from app.operation_executor import OperationExecutor\n",
    "from app.utils import Helper\n",
    "ops = OperationExecutor()\n",
    "\n",
    "path = r\"C:\\Users\\kaustubh.keny\\Projects\\JSON25\\scrape_output\\cache\\1109_1009_compare\\compare-080925.json\"\n",
    "result = Helper.load_json(path)\n",
    "ops.generate_sorted_excel_report(result,output_path=\"Despoit_Rate_Report.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
