{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b7f28e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import time, base64,ssl, os, re,json5, pprint,random , math, hashlib, inspect,requests\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.by import By                   # Locators (ID, CLASS_NAME, XPATH, etc.)\n",
    "from selenium.webdriver.support.ui import WebDriverWait       # Waits for elements to appear\n",
    "from selenium.webdriver.support import expected_conditions as EC  # Conditions like \"visible\", \"clickable\"\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from urllib.parse import urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "from app.utils import Helper\n",
    "from app.operation_executor import OperationExecutor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from io import StringIO\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8c7b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from playwright.sync_api import sync_playwright\n",
    "\n",
    "with sync_playwright() as p:\n",
    "    browser = p.chromium.launch(headless=False)  # headless=True runs invisibly\n",
    "    page = browser.new_page()\n",
    "\n",
    "    page.goto(\"https://www.google.com\")\n",
    "    page.fill(\"input[name='q']\", \"borgir\")\n",
    "    page.press(\"input[name='q']\", \"Enter\")\n",
    "\n",
    "    page.wait_for_timeout(3000)  # 3 seconds to admire borgir results\n",
    "    browser.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d591581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are less than three cols, skipping\n",
      "Excel saved: Orders_example_All.xlsx\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def parse_html_table(html):\n",
    "    \"\"\"Parse one HTML table into DataFrame with Date, Subject, Remarks, Link.\"\"\"\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    table = soup.find(\"table\")\n",
    "    if not table:\n",
    "        return None\n",
    "\n",
    "    rows = []\n",
    "    for tr in table.select(\"tbody tr\"):\n",
    "        cols = tr.find_all(\"td\")\n",
    "        if len(cols) < 3:\n",
    "            print(\"There are less than three cols, skipping\")\n",
    "            continue\n",
    "\n",
    "        date = cols[0].get_text(strip=True)\n",
    "        remarks = cols[2].get_text(strip=True)\n",
    "\n",
    "        a = cols[1].find(\"a\")\n",
    "        subject = a.get_text(\" \", strip=True) if a else cols[1].get_text(strip=True)\n",
    "        link = \"\"\n",
    "        if a and a.get(\"onclick\"):\n",
    "            onclick = a[\"onclick\"]\n",
    "            if \"newwindow1\" in onclick:\n",
    "                start = onclick.find(\"'\") + 1\n",
    "                end = onclick.rfind(\"'\")\n",
    "                link = onclick[start:end]\n",
    "\n",
    "        rows.append([date, subject, remarks, link])\n",
    "\n",
    "    return pd.DataFrame(rows, columns=[\"Date\", \"Subject\", \"Remarks\", \"Link\"])\n",
    "\n",
    "def clean_sheet_name(name, fallback=\"Sheet\"):\n",
    "    safe = re.sub(r'[\\[\\]\\:\\*\\?\\/\\\\]', \"_\", str(name))\n",
    "    return safe[:31] if safe else fallback\n",
    "\n",
    "def cache_to_excel(cache_file, excel_out=\"Orders_All.xlsx\"):\n",
    "    with open(cache_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    used_names = {}  # track counts per sheet name\n",
    "\n",
    "    with pd.ExcelWriter(excel_out, engine=\"xlsxwriter\") as writer:\n",
    "        for scraped in data[\"records\"][0][\"scraped_data\"]:\n",
    "            if not scraped.get(\"data_present\"):\n",
    "                continue\n",
    "\n",
    "            action = scraped.get(\"action\", \"unknown\")\n",
    "            webpage = scraped.get(\"webpage\", \"\")\n",
    "            for resp in scraped.get(\"response\", []):\n",
    "                if resp.get(\"type\") != \"table_html\":\n",
    "                    continue\n",
    "\n",
    "                df = parse_html_table(resp[\"value\"])\n",
    "                if df is None or df.empty:\n",
    "                    continue\n",
    "\n",
    "                titles = resp.get(\"title\", [])\n",
    "                base_name = clean_sheet_name(titles[-1] if titles else action)\n",
    "\n",
    "                count = used_names.get(base_name, 0) + 1\n",
    "                used_names[base_name] = count\n",
    "                sheet_name = base_name if count == 1 else clean_sheet_name(f\"{base_name}_{count}\")\n",
    "\n",
    "\n",
    "                meta_rows = [[\"Action\", action], [\"Webpage\", webpage]]\n",
    "                for i, t in enumerate(titles, start=1):\n",
    "                    meta_rows.append([f\"Header{i}\", t])\n",
    "                meta = pd.DataFrame(meta_rows, columns=[\"Meta\", \"Value\"])\n",
    "                meta.to_excel(writer, sheet_name=sheet_name, index=False, startrow=0)\n",
    "\n",
    "                df_visible = df[[\"Date\", \"Subject\", \"Remarks\",\"Link\"]]\n",
    "                startrow = len(meta) + 2\n",
    "                df_visible.to_excel(writer, sheet_name=sheet_name, index=False, startrow=startrow)\n",
    "\n",
    "                # ws = writer.sheets[sheet_name]\n",
    "                # for i, (txt, url) in enumerate(zip(df[\"Subject\"], df[\"Link\"]), start=startrow+1):\n",
    "                #     if url:\n",
    "                #         ws.write_url(i, 1, url, string=txt)\n",
    "\n",
    "    print(f\"Excel saved: {excel_out}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "path = r\"C:\\Users\\kaustubh.keny\\Projects\\JSON25\\scrape_output\\cache\\2025-09-23\\25-09-23T15-32-09_cache.json\"\n",
    "cache_to_excel(path, \"Orders_example_All.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e62817d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.operation_executor import OperationExecutor\n",
    "from app.utils import Helper\n",
    "path = r\"C:\\Users\\rando\\Office Projects\\outputs\\scrape_output\\cache\\2025-09-23\\25-09-23T22-31-06_cache.json\"\n",
    "data = Helper.load_json(path)\n",
    "OperationExecutor.generate_cache_doc_report(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98a9410e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF URL: https://punjabandsind.bank.in/system/uploads/bulk_deposit_rate/2025100413032313708.pdf\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://punjabandsind.bank.in/module/bulk-deposit-rates\")\n",
    "\n",
    "# find the anchor or button linking to PDF\n",
    "link = driver.find_element(By.PARTIAL_LINK_TEXT, \".pdf\")\n",
    "pdf_url = link.get_attribute(\"href\")\n",
    "\n",
    "print(\"PDF URL:\", pdf_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03340adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF URL: https://punjabandsind.bank.in/system/uploads/bulk_deposit_rate/2025100413032313708.pdf\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://punjabandsind.bank.in/module/bulk-deposit-rates\")\n",
    "\n",
    "# find the anchor or button linking to PDF\n",
    "link = driver.find_element(By.PARTIAL_LINK_TEXT, \".pdf\")\n",
    "pdf_url = link.get_attribute(\"href\")\n",
    "\n",
    "print(\"PDF URL:\", pdf_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2eff7757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>Processing Bank of Baroda\n"
     ]
    }
   ],
   "source": [
    "from app.operation_executor import OperationExecutor\n",
    "from app.utils import Helper\n",
    "from urllib.parse import urljoin\n",
    "import os\n",
    "ops = OperationExecutor()\n",
    "path = r\"C:\\Users\\kaustubh.keny\\Projects\\JSON25\\scrape_output\\cache\\2025-09-26\\25-09-26T15-44-07_cache.json\"\n",
    "# path = r\"25-09-08T18-33-09_cache.json5\"\n",
    "path = r\"C:\\Users\\kaustubh.keny\\Projects\\JSON25\\scrape_output\\cache\\2025-09-26\\25-09-26T15-39-53_cache.json\"\n",
    "\n",
    "    \n",
    "root_file = os.path.basename(path).split(\"_\")[0]\n",
    "function_to_execute = {\n",
    "\"primary\":[[\"normalize_df\",\"value\",\"norm_table\",\"table_html\"],[\"sha1\",\"value\",\"SHA_ONE\",\"pdf\"]],\n",
    "\"secondary\":[[\"sha1\",\"norm_table\",\"SHA_ONE\"]],\n",
    "}\n",
    "try:\n",
    "    data = Helper.load_json(path, typ=\"json5\")\n",
    "    processed_data = ops.runner(data,function_to_execute)\n",
    "    Helper.save_json(processed_data,f\"{root_file}_process.json\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError in Processing.. Skipping, {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "945fdec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Despoit_Rate_Report.xlsx'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from app.operation_executor import OperationExecutor\n",
    "from app.utils import Helper\n",
    "ops = OperationExecutor()\n",
    "\n",
    "v1 = r\"25-09-26T15-44-07_process.json\"\n",
    "v2 = r\"25-09-26T15-39-53_process.json\"\n",
    "yesterday = Helper.load_json(v2)\n",
    "today = Helper.load_json(v1)\n",
    "\n",
    "result = ops.process_comparison(yesterday,today,key=\"SHA_ONE\")\n",
    "Helper.save_json(result, \"compare-080925.json\")\n",
    "\n",
    "ops.generate_sorted_excel_report(result,output_path=\"Despoit_Rate_Report.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87a26a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Despoit_Rate_Report.xlsx'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from app.operation_executor import OperationExecutor\n",
    "from app.utils import Helper\n",
    "ops = OperationExecutor()\n",
    "\n",
    "path = r\"C:\\Users\\kaustubh.keny\\Projects\\JSON25\\scrape_output\\cache\\1109_1009_compare\\compare-080925.json\"\n",
    "result = Helper.load_json(path)\n",
    "ops.generate_sorted_excel_report(result,output_path=\"Despoit_Rate_Report.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
